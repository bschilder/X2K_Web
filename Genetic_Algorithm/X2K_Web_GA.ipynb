{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# X2K_Web Genetic Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X2K Web API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import http.client\n",
    "import json\n",
    "\n",
    "all_x2k_options = {\n",
    "    'TF-target gene background database used for enrichment': [\n",
    "        'ChEA 2015',\n",
    "        'ENCODE 2015',\n",
    "        'ChEA & ENCODE Consensus',\n",
    "        'Transfac and Jaspar',\n",
    "        'ChEA 2016',\n",
    "        'ARCHS4 TFs Coexp',\n",
    "        'CREEDS',\n",
    "        'Enrichr Submissions TF-Gene Coocurrence',\n",
    "    ],\n",
    "    'kinase interactions to include': [#kea 2016\n",
    "        'kea 2018',\n",
    "        'ARCHS4',\n",
    "        'iPTMnet',\n",
    "        'NetworkIN',\n",
    "        'Phospho.ELM',\n",
    "        'Phosphopoint',\n",
    "        'PhosphoPlus',\n",
    "        'MINT',\n",
    "    ],\n",
    "    'enable_ppi': [\n",
    "        'ppid',\n",
    "        'Stelzl',\n",
    "        'IntAct',\n",
    "        'MINT',\n",
    "        'BioGRID',\n",
    "        'Biocarta',\n",
    "        'BioPlex',\n",
    "        'DIP',\n",
    "        'huMAP',\n",
    "        'InnateDB',\n",
    "        'KEGG',\n",
    "        'SNAVI',\n",
    "        'iREF',\n",
    "        'vidal',\n",
    "        'BIND',\n",
    "        'figeys',\n",
    "        'HPRD',\n",
    "    ],\n",
    "    'max_number_of_interactions_per_article':  {\"10\":15, \"01\":50, \"11\":200, \"00\":1000000},\n",
    "    'max_number_of_interactions_per_protein': {\"10\":50, \"01\":100, \"11\":200, \"00\":500},\n",
    "    'min_network_size': {\"10\":1, \"01\":10, \"11\":50, \"00\":100},\n",
    "    'min_number_of_articles_supporting_interaction': {\"10\":0, \"01\":1, \"11\":5, \"00\":10},\n",
    "    'path_length': {\"0\":1, \"1\":2},\n",
    "    'included organisms in the background database': {\"10\": \"human\", \"01\": \"mouse\", \"11\": \"both\", \"00\": \"RESHUFFLE\"},\n",
    "}\n",
    "\n",
    "def run_X2K(input_genes, x2k_options={}):\n",
    "    # Open HTTP connection\n",
    "    conn = http.client.HTTPConnection(\"amp.pharm.mssm.edu\") #\n",
    "    #conn = http.client.HTTPConnection(\"localhost:8080\", timeout=20)\n",
    "    # Get default options\n",
    "    default_options = {'text-genes': '\\n'.join(input_genes), 'included_organisms': 'both', 'included_database': 'ChEA 2015',\n",
    "                       'path_length': 2, 'minimum network size': 50, 'min_number_of_articles_supporting_interaction': 2,\n",
    "                       'max_number_of_interactions_per_protein': 200, 'max_number_of_interactions_per_article': 100,\n",
    "                       'biocarta': True, 'biogrid': True, 'dip': True, 'innatedb': True, 'intact': True, 'kegg': True, 'mint': True,\n",
    "                       'ppid': True, 'snavi': True, 'number_of_results': 50, 'sort_tfs_by': 'combined score', 'sort_kinases_by': 'combined score',\n",
    "                       'kinase interactions to include': 'kea 2018'}\n",
    "    # Update options\n",
    "    for key, value in x2k_options.items():\n",
    "        if key in default_options.keys() and key != 'text-genes':\n",
    "            default_options.update({key: value})\n",
    "    # Get payload\n",
    "    boundary = \"----WebKitFormBoundary7MA4YWxkTrZu0gW\"\n",
    "    payload = ''.join(['--'+boundary+'\\r\\nContent-Disposition: form-data; name=\\\"{key}\\\"\\r\\n\\r\\n{value}\\r\\n'.format(**locals()) for key, value in default_options.items()])+'--'+boundary+'--'\n",
    "    # Get Headers\n",
    "    headers = {\n",
    "        'content-type': \"multipart/form-data; boundary=\"+boundary,\n",
    "        'cache-control': \"no-cache\",\n",
    "    }\n",
    "    # Initialize connection\n",
    "    conn.request(\"POST\", \"/X2K/api\", payload, headers)\n",
    "    # Get response\n",
    "    res = conn.getresponse()\n",
    "    # Read response\n",
    "    data = res.read().decode('utf-8')\n",
    "    # Convert to dictionary\n",
    "    x2k_results = {key: json.loads(value) if key != 'input' else value for key, value in json.loads(data).items()}\n",
    "    # Clean results\n",
    "    x2k_results['ChEA'] = x2k_results['ChEA']['tfs']\n",
    "    x2k_results['G2N'] = x2k_results['G2N']['network']['nodes']\n",
    "    x2k_results['KEA'] = x2k_results['KEA']['kinases']\n",
    "    x2k_results['X2K'] = x2k_results['X2K']['network']\n",
    "    # Return results\n",
    "    return x2k_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Standardize genes to HGNC symbols\n",
    "mapping = pd.read_table('../X2K_Summaries/General_Resources/Moshe_mapping/mappingFile_2017.txt', header=None)\n",
    "greekLetters = pd.read_csv('../X2K_Summaries/General_Resources/GreekLetter_Converter.csv', names=['Greek', 'Abbrev'], header=0 )\n",
    "greekLetters = greekLetters.apply(lambda x: x.str.strip('\\xa0'))\n",
    "\n",
    "def standardizeGeneSymbol(gene):\n",
    "    if gene.__contains__('AURORA'):\n",
    "        HGNC = 'AURK' + gene[-1]\n",
    "    elif any(substring in gene for substring in greekLetters['Greek']):\n",
    "        for letter in greekLetters['Greek']:\n",
    "            LETTER = letter.upper()\n",
    "            if gene.__contains__(LETTER):\n",
    "                HGNC = gene.replace(LETTER, greekLetters.loc[greekLetters['Greek']==letter,'Abbrev'].values[0] )\n",
    "    else:\n",
    "        HGNC = gene\n",
    "    if HGNC in mapping[0]:\n",
    "        HGNC = mapping.iloc[mapping[0]==HGNC, 1] \n",
    "    return HGNC\n",
    "\n",
    "\n",
    "def parse_GEO_line(line):\n",
    "    lineSp = line.split('\\t')\n",
    "    expt_name = lineSp[0]\n",
    "    genes = [str(x.strip(',1.0')) for x in lineSp[2:-1]]\n",
    "    return expt_name, genes\n",
    "\n",
    "def prepare_options_for_x2k(input_genes, x2k_parameters):\n",
    "    options=x2k_parameters.copy()\n",
    "    for param in options:\n",
    "        options[param] = options[param]['selection']\n",
    "    # Add input_genes\n",
    "    options['text-genes'] = input_genes\n",
    "    # Convert ppi into enable flags\n",
    "    for ppi in options['enable_ppi']:\n",
    "        options['enable_' + ppi] = 'true'\n",
    "    del options['enable_ppi']\n",
    "    # Convert any lists\n",
    "    return {\n",
    "        k: '\\n'.join(v) if type(v) == list else str(v)\n",
    "        for k, v in options.items() \n",
    "    }\n",
    "\n",
    "def translateDatabases(binaryString_segment, _dbs):\n",
    "    selection = []\n",
    "    for i, bit in enumerate(binaryString_segment):\n",
    "        if bit == \"1\":\n",
    "            selection.append(_dbs[i])\n",
    "    return selection\n",
    "\n",
    "# binaryString = RS_population[0]\n",
    "def binary_to_parameters(binaryString):\n",
    "    x2k_parameters={}\n",
    "    stringCount=0\n",
    "    for key in all_x2k_options:\n",
    "        parameter = all_x2k_options[key]\n",
    "        if type(parameter)==list:\n",
    "            bitSegment = binaryString[stringCount:stringCount + len(parameter)]\n",
    "            selection = translateDatabases(bitSegment, parameter)\n",
    "            x2k_parameters[key] = {'selection':selection, 'bits':bitSegment}\n",
    "            stringCount += len(bitSegment)\n",
    "        if type(parameter)==dict:\n",
    "            bitLength = len(list(parameter.keys())[0])\n",
    "            bits = binaryString[stringCount:stringCount + bitLength]\n",
    "            selection = parameter[bits]\n",
    "            x2k_parameters[key] = {'selection':selection, 'bits':bits}\n",
    "            stringCount += bitLength\n",
    "    return x2k_parameters\n",
    "\n",
    "def parameters_to_binary(x2k_parameters):\n",
    "    newBinary=[]\n",
    "    for key in all_x2k_options:\n",
    "        newBinary.append( x2k_parameters[key]['bits'] )\n",
    "    return ''.join(newBinary)\n",
    "\n",
    "\"\"\"\n",
    "params = binary_to_parameters(binaryString)\n",
    "recoveredBinary = parameters_to_binary(params)\n",
    "binaryString == recoveredBinary\n",
    "\"\"\"\n",
    "\n",
    "def reshuffle_binary(binaryString):\n",
    "    import random\n",
    "    new_options = binary_to_parameters(binaryString)\n",
    "    for param in new_options:\n",
    "        paramSelection = new_options[param]['selection']\n",
    "        while paramSelection == \"RESHUFFLE\":\n",
    "            binarySelection = random.choice( list(all_x2k_options[param]) )\n",
    "            paramSelection = all_x2k_options[param][binarySelection]\n",
    "            new_options[param] = {'bits':binarySelection,'selection':paramSelection}\n",
    "    newBinary = parameters_to_binary(new_options)\n",
    "    return newBinary\n",
    "    \n",
    " \n",
    "############ Parallel processing of X2K ############\n",
    "\n",
    "    # Correct binaryString\n",
    "\n",
    "def prepare_all_x2k_inputs(newBinary, gmtLimit=False):\n",
    "    # Open GMT\n",
    "    with open('Genetic_Algorithm/testgmt/'+os.listdir('Genetic_Algorithm/testgmt')[0]) as gmt_file:\n",
    "        gmtLines = gmt_file.readlines()\n",
    "    if gmtLimit!=False:\n",
    "        gmtLines = gmtLines[0:gmtLimit]\n",
    "    # Prepare inputs\n",
    "    def prepare_one_x2k_input(line, newBinary):\n",
    "        expt_name, input_genes = parse_GEO_line(line)\n",
    "        ## Standardize input genes\n",
    "        input_genes = [standardizeGeneSymbol(g) for g in input_genes]\n",
    "        x2k_parameters = binary_to_parameters(newBinary)\n",
    "        x2k_options = prepare_options_for_x2k(input_genes, x2k_parameters)\n",
    "        x2k_input = {'options':x2k_options, 'expt_name':expt_name}\n",
    "        return x2k_input\n",
    "    all_x2k_inputs = list(map(prepare_one_x2k_input, gmtLines, [newBinary]*len(gmtLines)))\n",
    "    return all_x2k_inputs\n",
    "\n",
    "def run_X2K_once(x2k_input):\n",
    "    x2k_options = x2k_input['options']\n",
    "    input_genes = x2k_options['text-genes'].split('\\n')\n",
    "    expt_name = x2k_input['expt_name']\n",
    "    try:\n",
    "        x2k_results = run_X2K(input_genes=input_genes, x2k_options=x2k_options)\n",
    "    except:\n",
    "        expt_name='FAIL'; x2k_results='NA'\n",
    "    return {'experiment':expt_name,'results':x2k_results}\n",
    "\n",
    "def parallel_x2k_results(all_x2k_inputs, threadPool_size=20):\n",
    "    # ********* Parallelize X2K across experiments ********* #\n",
    "    # ****************************************************** #\n",
    "    # import time\n",
    "    # start = time.time()\n",
    "    from multiprocessing.dummy import Pool as ThreadPool\n",
    "    pool = ThreadPool(threadPool_size)\n",
    "    raw_x2k_results = pool.map(run_X2K_once, all_x2k_inputs)\n",
    "    pool.close() \n",
    "    pool.join() \n",
    "    # end = time.time()\n",
    "    # print(end - start)\n",
    "    \n",
    "    # Post-process pooled results\n",
    "    allExpts_x2k_results={}\n",
    "    for dict in raw_x2k_results:\n",
    "        expt_name = dict['experiment']\n",
    "        if expt_name!='FAIL':\n",
    "            allExpts_x2k_results[expt_name] = dict['results']  \n",
    "   \n",
    "    return allExpts_x2k_results\n",
    "\"\"\"\n",
    "binaryString=createPopulation(1)[0]\n",
    "population=createPopulation(10)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run X2K GA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GA Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "# 0. Create initial population\n",
    "###################################\n",
    "def stringLength():\n",
    "    string_length=0\n",
    "    for key in all_x2k_options:\n",
    "        parameter = all_x2k_options[key]\n",
    "        if type(parameter) ==list:\n",
    "            string_length+=len(parameter) # All possible databases\n",
    "        if type(parameter)==dict:\n",
    "            string_length += len(next(iter(parameter.keys()))) # Random bit option\n",
    "    return  string_length\n",
    "\n",
    "def createPopulation(popSize):\n",
    "    from random import choice\n",
    "    binaryStringLength = stringLength()\n",
    "    populationinit = []\n",
    "    for i in range(popSize):\n",
    "        populationinit.append(''.join(choice(('0', '1')) for _ in range(binaryStringLength)) )\n",
    "        print(populationinit[i])\n",
    "    return populationinit\n",
    "\n",
    "###################################\n",
    "# 1. Calculate fitness\n",
    "###################################\n",
    "def pvalue_matrix(all_x2k_results, dataType='KEA' ):\n",
    "    nameKey = {'ChEA':'simpleName','KEA':'name', 'G2N':'name'}\n",
    "    ## dict_keys(['X2K', 'ChEA', 'KEA', 'G2N', 'input', 'Experiment', 'x2k_options', 'binaryString'])\n",
    "    # Experiment -> Kinase -> kinase results\n",
    "    pvalDict={} \n",
    "    for expt in all_x2k_results:\n",
    "        results = all_x2k_results[expt][dataType]\n",
    "        if dataType == 'G2N':\n",
    "            for g in results:\n",
    "                g['name'] = g['name'].split(\"-\")[0]\n",
    "        predictedKinases = [y[nameKey[dataType]] for y in results]\n",
    "        predictedPvals = [y['pvalue'] for y in results]\n",
    "        # if replaceNAs==True:\n",
    "        #     predictedPvals = [1.0 if math.isnan(x) else x for x in predictedPvals]\n",
    "        pvalDict[expt] = dict(zip(predictedKinases, predictedPvals))\n",
    "     return pvalDict\n",
    "\n",
    "# Calculate fitness every each individual in pop\n",
    "def population_fitness(gen, population, fitnessDict, fitness_method='target_shuffled_difference', gmtLimit=False, threadPool_size=20):\n",
    "    population_results={}\n",
    "    newFitnessDict = fitnessDict.copy()\n",
    "    for i,binaryString in enumerate(population):\n",
    "        import time  \n",
    "        start = time.time()\n",
    "        unique_id = \"ind\"+str(i)+\"_gen\"+str(gen)\n",
    "        # Pull from fitnessDict if newBinary previously seen\n",
    "        if binaryString in newFitnessDict:\n",
    "            print(\"Pulling info from fitnessDict\")\n",
    "            dict_pull = newFitnessDict[binaryString].copy()\n",
    "            dict_pull['generation'] = gen\n",
    "            population_results[unique_id] = dict_pull\n",
    "            print(unique_id+\": Fitness = \"+str(round(dict_pull['fitness'],3))+\" [pulled from fitnessDict]\")\n",
    "        else:\n",
    "            newBinary = reshuffle_binary(binaryString)\n",
    "            all_x2k_inputs = prepare_all_x2k_inputs(newBinary, gmtLimit)\n",
    "            all_x2k_results = parallel_x2k_results(all_x2k_inputs=all_x2k_inputs, threadPool_size=threadPool_size)\n",
    "            CHEA_pvalDict = pvalue_matrix(all_x2k_results, 'ChEA')\n",
    "            KEA_pvalDict = pvalue_matrix(all_x2k_results, 'KEA')\n",
    "            G2N_pvalDict = pvalue_matrix(all_x2k_results, 'G2N')\n",
    "            fitness, Z_shuffled_ranks, Z_target_ranks, RAW_shuffled_ranks, RAW_target_ranks = eval(fitness_method)(KEA_pvalDict)\n",
    "            population_results[unique_id] = {'generation':gen, 'binaryString':binaryString, 'newBinary':newBinary,\n",
    "                                             'fitness':fitness, 'KEA_results':KEA_pvalDict, 'CHEA_results':CHEA_pvalDict,\n",
    "                                             'G2N_results':G2N_pvalDict,\n",
    "                                             'Zscore_shuffled_ranks':Z_shuffled_ranks, 'Zscore_target_ranks':Z_target_ranks,\n",
    "                                             'RAW_shuffled_ranks':RAW_shuffled_ranks, 'RAW_target_ranks':RAW_target_ranks}\n",
    "            print(unique_id+\": Fitness = \"+str(round(fitness,3)))\n",
    "            #Add to fitness dictionary\n",
    "            newFitnessDict[newBinary] = population_results[unique_id]\n",
    "        end = time.time()\n",
    "        print(\"One individual took: \"+str(round(end - start, 2))+\"s\")\n",
    "    return population_results, newFitnessDict\n",
    "\n",
    "\"\"\"\n",
    "population=createPopulation(5)\n",
    "pop_fitness_results, newFitnessDict = population_fitness(gen=0, fitnessDict={}, population=population, gmtLimit=15)\n",
    "\"\"\"\n",
    "\n",
    "###################################\n",
    "# 2. Select fittest individuals\n",
    "###################################\n",
    "\n",
    "def selectFittest(topNum, pop_fitness_results, selection_method):\n",
    "    import pandas as pd\n",
    "    fitDF = pd.DataFrame(pop_fitness_results).T\n",
    "    def print_fittest(fittestDF):\n",
    "        fitnesses = fittestDF[\"fitness\"].values\n",
    "        print('Top fitnesses: '+str([round(x,3) for x in fitnesses]))\n",
    "    \n",
    "    if selection_method == 'fitnessProportional':\n",
    "        fittestDF = fitDF.sort_values(by='fitness', ascending=False).iloc[:topNum,:].copy()\n",
    "        print_fittest(fittestDF)\n",
    "        return fittestDF\n",
    "    # Tournament selection (less stringent)\n",
    "    ## Split the population into equal subgroups, and then select the fittest individual from each group\n",
    "    if selection_method == 'Tournament':\n",
    "        fittestDF=pd.DataFrame()\n",
    "        if fitDF.shape[0] % topNum!=0:\n",
    "            print(\"Tournament selection requires that populationSize/topNum and childrenPerGeneration/topNum are both whole numbers.\")\n",
    "        subsetSize = int( fitDF.shape[0] / topNum )\n",
    "        for t in range(topNum):\n",
    "            subDF = fitDF.sample(n=subsetSize, replace=False)\n",
    "            fittestDF = fittestDF.append( subDF.sort_values(by=['fitness'], ascending=False).iloc[0,:].copy())\n",
    "        print_fittest(fittestDF)\n",
    "        return fittestDF\n",
    "    if selection_method == 'mixedTournament':\n",
    "        if fitDF.shape[0]%topNum!=0 or topNum%2!=0:\n",
    "            print(\"WARNING:: Tournament selection requires that populationSize/topNum, \\n\"\n",
    "                  +\"childrenPerGeneration/topNum, and topNum/2 to be whole numbers.\")\n",
    "        topNumHalf = int(topNum/2)\n",
    "        sortedDF = fitDF.sort_values(by=['fitness'], ascending=False).copy()\n",
    "        # The first half of the new pop are the fittest parents overall\n",
    "        fittestDF = sortedDF.iloc[:topNumHalf, :].copy()\n",
    "        # Then run Tournament selection on the rest of the population to get the other half of the new pop\n",
    "        everybodyElse = sortedDF.iloc[topNumHalf:, :].copy()\n",
    "        subsetSize = int(everybodyElse.shape[0] / topNumHalf)\n",
    "        for t in range(topNumHalf):\n",
    "            subDF = everybodyElse.sample(n=subsetSize, replace=False)\n",
    "            fittestDF = fittestDF.append(subDF.sort_values(by=['fitness'], ascending=False).iloc[0, :].copy())\n",
    "        print_fittest(fittestDF)\n",
    "        return fittestDF\n",
    "    else:\n",
    "        print(\"Use viable 'selectionMethod'\")\n",
    "    \n",
    "\"\"\"\n",
    "fittestDF = selectFittest(topNum=4, pop_fitness_results=pop_fitness_results, selection_method='Fitness-proportional')\n",
    "\"\"\"\n",
    "\n",
    "def simple_selectFittest(topNum, pop_fitness_results):\n",
    "    import pandas as pd\n",
    "    fitDF = pd.DataFrame(pop_fitness_results).copy().T \n",
    "    def print_fittest(fittestDF):\n",
    "        fitnesses = fittestDF[\"fitness\"].values\n",
    "        print('Top fitnesses: '+str([round(x,3) for x in fitnesses]))\n",
    "    fittestDF = fitDF.sort_values(by='fitness', ascending=False).iloc[:topNum,:].copy()\n",
    "    print_fittest(fittestDF)\n",
    "    return fittestDF\n",
    "    \n",
    "\n",
    "###################################\n",
    "# 3. Crossover/breed fittest\n",
    "###################################\n",
    "###################################\n",
    "# 4. Introduce random mutations\n",
    "###################################\n",
    "def createChild(individual1, individual2, crossover_points, crossover_locations=\"evenlyDistributed\"):\n",
    "    if crossover_locations==\"evenlyDistributed\":\n",
    "        chunkSize = int(len(individual1) / (crossover_points+1))\n",
    "        ind1Split = [individual1[i:i + chunkSize] for i in range(0, len(individual1), chunkSize)]\n",
    "        ind2Split = [individual2[i:i + chunkSize] for i in range(0, len(individual2), chunkSize)]\n",
    "    elif crossover_locations=='random':\n",
    "        from random import sample\n",
    "        cutpoints = sorted(sample(range(1, len(individual1)-1), crossover_points)) # randomly generate n non-overlapping numbers\n",
    "        def splitParent(parent, cutpoints):\n",
    "            indSplit=[]\n",
    "            for i,num in enumerate(cutpoints):\n",
    "                #print(\"**Cutpoint index= \"+str(i))\n",
    "                if i == 0: # If it's the first cutpoint, take all values up to the first index+1\n",
    "                    start = 0\n",
    "                    end = num\n",
    "                else:\n",
    "                    start = cutpoints[i-1]\n",
    "                    end = num\n",
    "                segment = parent[start:end]\n",
    "                #print(\"Cutpoint= \" + str(start) + \" : \" + str(end))\n",
    "                #print(\"------- \"+segment+\" -------\")\n",
    "                indSplit.append(segment)\n",
    "            # Add the very last segment\n",
    "            indSplit.append(parent[cutpoints[-1]:])\n",
    "            return indSplit\n",
    "        ind1Split = splitParent(individual1, cutpoints)\n",
    "        ind2Split = splitParent(individual2, cutpoints)\n",
    "    # Put together the new child\n",
    "    from random import random\n",
    "    childFragments=[]\n",
    "    for fragment in range(len(ind1Split)):\n",
    "        if int(100*random()) < 50: # Just randomly picks from ParentA or ParentB for each individual parameter\n",
    "            childFragments.append(ind1Split[fragment])\n",
    "        else:\n",
    "            childFragments.append(ind2Split[fragment])\n",
    "    child = \"\".join(childFragments)\n",
    "    return child\n",
    "\"\"\"\n",
    "individual1, individual2 = fittestDF['newBinary'][:2,]\n",
    "child = createChild(individual1, individual2, crossoverPoints=8, crossoverLocations=\"evenlyDistributed\")\n",
    "\"\"\"\n",
    "\n",
    "def mutateChild(child, mutationRate):\n",
    "    from random import random\n",
    "    mutant = ''\n",
    "    for bit, val in enumerate(child):\n",
    "        rando = random()\n",
    "        if rando <= mutationRate and val == '1':\n",
    "            mutant = str(str(mutant) + '0')\n",
    "        elif rando <= mutationRate and val == '0':\n",
    "            mutant = str(str(mutant) + '1')\n",
    "        else:\n",
    "            mutant = str(str(mutant) + str(val))\n",
    "    return mutant\n",
    "\n",
    "def createChildren(numberOfChildren, fittestDF, mutationRate, crossover_points, crossover_locations):\n",
    "    from random import random\n",
    "    fittest = fittestDF['newBinary'].tolist()\n",
    "    #breedingChances = []\n",
    "    # Add noise to fitness score?\n",
    "    # for b in range(len(Fittest)):\n",
    "    #     breedingChances.append(np.random.uniform(1 + breedingVariation, 1 - breedingVariation) * int(fittestFitness[b]))\n",
    "    #     topBreeders = [x for _, x in sorted(zip(breedingChances, Fittest), reverse=True)]\n",
    "    # Breed n times\n",
    "    # 'Once you're in, you're in'. After selecting the top fittest individuals, it doesn't matter who is fitter within that group: everyone breeds with everyone else randomly\n",
    "    children = []\n",
    "    for i in range(numberOfChildren):\n",
    "        ind1 = int(random()*len(fittest))\n",
    "        ind2 = int(random()*len(fittest))\n",
    "        child = createChild(fittest[ind1], fittest[ind2], crossover_points, crossover_locations)\n",
    "        # MUTATE the children!\n",
    "        child = mutateChild(child, mutationRate)\n",
    "        children.append(child)\n",
    "    return children\n",
    "\"\"\"\n",
    "children = createChildren( 8, fittestDF, 0.01, crossoverLocations=\"evenlyDistributed\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitness Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# Fitness Support Functions\n",
    "############################ \n",
    "def values_to_ranks(valuesDF, ascending=False):\n",
    "    Ranks={}\n",
    "    # assign ranks based on given value (could be pvalue, -log(pvalue), ranks, etc)\n",
    "    for col in valuesDF:\n",
    "        # Since zscore comes from -log(pvalue), flip the rank order so that low numbered ranks are still the best\n",
    "        orderedCol = valuesDF[col].sort_values(ascending=ascending)\n",
    "        # Shuffle order of 0s\n",
    "        nonNAs = orderedCol.loc[~orderedCol.isna()]\n",
    "        try:\n",
    "            NAs = orderedCol.loc[orderedCol.isna()].sample(frac=1)\n",
    "        except:\n",
    "            NAs = pd.Series(dtype=float)\n",
    "        shuffledCol = pd.concat([nonNAs, NAs])\n",
    "        # Assign ranks\n",
    "        newRanks = pd.Series(data=range(0,len(shuffledCol)), name=col, index=shuffledCol.index)\n",
    "        newRanks.sort_index(inplace=True) # Sort by index\n",
    "        Ranks[col] = dict(zip(newRanks.index, newRanks.values)) \n",
    "    return pd.DataFrame(Ranks)\n",
    "\n",
    "import math\n",
    "def values_to_zscores(DF, dropNAs=False):\n",
    "    if dropNAs==True:\n",
    "        # Drop all rows that ONLY have (0). Never appeared across any experiment\n",
    "        # Keeping the all 0s messes up the zscore\n",
    "        df = DF[(DF.T != math.nan).any()]\n",
    "    else:\n",
    "        df = DF.copy()\n",
    "    zScoreDF = df.apply(lambda x: (x-x.mean()) / x.std(ddof=0) ) \n",
    "    return zScoreDF\n",
    "\n",
    "def scaled_ranks(DFstack):\n",
    "    scaledDF = DFstack.copy()\n",
    "    scaledDF['Rank'] -= scaledDF['Rank'].min() \n",
    "    scaledDF['Rank'] /= scaledDF['Rank'].max()\n",
    "    return scaledDF\n",
    "\n",
    "def clearTestGMT():\n",
    "    import os\n",
    "    dir_name = \"Genetic_Algorithm/testgmt/\"\n",
    "    files = os.listdir(dir_name)\n",
    "    for item in files:\n",
    "        if item.endswith(\".txt\") or item.endswith(\".gmt\"):\n",
    "            os.remove(os.path.join(dir_name, item))\n",
    "\n",
    "import numpy as np\n",
    "def negLog(pval_matrix):\n",
    "    negLog_matrix = -np.log(pval_matrix)\n",
    "    return negLog_matrix\n",
    "    \n",
    "############################\n",
    "# Fitness Functions\n",
    "############################ \n",
    "#from scipy.stats import kendalltau\n",
    "#from scipy.stats import spearmanr\n",
    "#from scipy.stats import entropy\n",
    "#from scipy.stats import kstest\n",
    "#from sklearn.metrics import mutual_info_score\n",
    "from scipy.stats import ranksums # Assumes sample independence. Use Wilcoxon signed-rank test instead when sample are dependent\n",
    "# Kendall and Spearman calculate rank correlations (the difference between 2 rank distributions), \n",
    "##  but don't distinguish between one being lower that the other (need to minimize Target ranks AND maximize difference between Target and Shuffle).\n",
    "## Can't use t-test because the distributions are often not normal.\n",
    "\n",
    "# Select ranking method\n",
    "def rank_stack(pval_dict, zscore, scaledRanks, standardize=True):\n",
    "    pval_matrix = pd.DataFrame(pval_dict)\n",
    "    negLog_matrix = negLog(pval_matrix)\n",
    "    if zscore==True:\n",
    "        zScores = values_to_zscores(negLog_matrix)\n",
    "        ranks = values_to_ranks(zScores)\n",
    "    else:\n",
    "        ranks = values_to_ranks(negLog_matrix)\n",
    "    \n",
    "    DFstack = ranks.stack().reset_index()\n",
    "    DFstack.columns = ['Kinase','Experiment','Rank']\n",
    "    if scaledRanks==True:\n",
    "        DFstack = scaled_ranks(DFstack)\n",
    "    # Standardize target kinases in cols\n",
    "    if standardize==True:\n",
    "        DFstack['Experiment'] = ['_'.join([standardizeGeneSymbol(x.split('_')[0])]+x.split('_')[1:]) for x in  DFstack['Experiment']]\n",
    "    return DFstack\n",
    "\n",
    "def shuffle_targets(DFstack, DFstack_target):\n",
    "    DFstack_shuffled = DFstack_target.copy()\n",
    "    DFstack_shuffled.loc[:,['Experiment','Rank']] = DFstack.sample(n=len(DFstack_target)).loc[:,['Experiment','Rank']].values\n",
    "    DFstack_shuffled.index = DFstack_shuffled['Kinase']\n",
    "    return DFstack_shuffled\n",
    "\n",
    "def stats_test(DFstack_shuffled, DFstack_target):\n",
    "        ## Calculate the difference between two rank distributions\n",
    "        #fitness = DFmerged['Rank_shuffled'].mean() - DFmerged['Rank_target'].mean()\n",
    "        #kendalltau(DFmerged['Rank_shuffled'], DFmerged['Rank_target'])\n",
    "        #spearmanr(DFmerged['Rank_shuffled'], DFmerged['Rank_target'])\n",
    "        #fitness = entropy(pk=DFmerged['Rank_target_prob'], qk=DFmerged['Rank_shuffled_prob'])\n",
    "        #kstest(DFmerged['Rank_shuffled'], DFmerged['Rank_target'])\n",
    "        Wilcoxon_ranksums = ranksums(DFstack_shuffled['Rank'], DFstack_target['Rank'])\n",
    "        stats_result = Wilcoxon_ranksums[0]\n",
    "        return stats_result\n",
    "\n",
    "def target_shuffled_difference(KEA_pvalDict):\n",
    "    # Iterate stats test\n",
    "    stats_results=[]; Z_shuffled_ranks=[]; Z_target_ranks=[]; RAW_shuffled_ranks=[]; RAW_target_ranks=[]\n",
    "    #for i in range(100):\n",
    "    def iterate_stats(KEA_pvalDict):\n",
    "        try:\n",
    "            # Zscore: stats, shuffled & target ranks\n",
    "            Zstack = rank_stack(KEA_pvalDict, zscore=True, scaledRanks=True)\n",
    "            Zstack_target = Zstack.loc[Zstack['Kinase']==Zstack['Experiment'].str.split('_').str[0]] # Returning way too few  \n",
    "            Zstack_shuffled = shuffle_targets(Zstack, Zstack_target)\n",
    "            stats_result = stats_test(Zstack_shuffled, Zstack_target)\n",
    "            stats_results.append(stats_result)\n",
    "            Z_shuffled_ranks.extend(Zstack_shuffled['Rank'].tolist())\n",
    "            Z_target_ranks.extend(Zstack_target['Rank'].tolist())\n",
    "            # Raw: stats, shuffled & target ranks\n",
    "            RAWstack = rank_stack(KEA_pvalDict, zscore=False, scaledRanks=True)\n",
    "            RAWstack_target = RAWstack.loc[RAWstack['Kinase']==RAWstack['Experiment'].str.split('_').str[0]] # Returning way too few  \n",
    "            RAWstack_shuffled = shuffle_targets(RAWstack, RAWstack_target) \n",
    "            RAW_shuffled_ranks.extend(RAWstack_shuffled['Rank'].tolist())\n",
    "            RAW_target_ranks.extend(RAWstack_target['Rank'].tolist())\n",
    "        except:\n",
    "            print('Could not conduct stats test.')    \n",
    "        #return {'stats_results':stats_results,'shuffled_ranks':all_shuffled_ranks,'target_ranks':all_target_ranks)\n",
    "    # results = [iterate_stats(KEA_pvalDict) for x in range(100)]\n",
    "    iterations=100\n",
    "    map_run = list(map(iterate_stats, [KEA_pvalDict]*iterations )) \n",
    "    fitness = sum(stats_results)/len(stats_results)\n",
    "    return fitness, Z_shuffled_ranks, Z_target_ranks, RAW_shuffled_ranks, RAW_target_ranks\n",
    "\n",
    "\"\"\"\n",
    "pval_dict = pvalue_matrix(all_x2k_results, 'KEA')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading all the GA results into memory at once slows down everything way too much. Instead, pull subsets from db\n",
    "\"\"\"\n",
    "import pickle\n",
    "GA_train = pickle.load( open( \"Genetic_Algorithm/GA_Results/GA_results_10gen_100inds.pkl\", \"rb\" ) )\n",
    "#GA_train = GA_resultsDict['GA_train']\n",
    "#GA_train=GA_resultsDict\n",
    "\"\"\"\n",
    "import pymongo\n",
    "\n",
    "def GAresults_to_JSON(GAresults_dict, GAset):\n",
    "    import json\n",
    "    json_path = 'Genetic_Algorithm/GA_Results/'+GAset['save_results']+'.json'\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(GAresults_dict, f)\n",
    " \n",
    "def connect_to_mongoDB(GA_result_name, deleteOldDB=False):\n",
    "    connection = pymongo.MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = connection.x2k_GA\n",
    "    db.collection_names(include_system_collections=False)\n",
    "    if deleteOldDB==True:\n",
    "        db[GA_result_name].drop() # remove collection\n",
    "    collection = db[GA_result_name]\n",
    "    return collection\n",
    "# collection = connect_to_mongoDB('GA_test')\n",
    "\n",
    "def GAresults_to_MongoDB(GAresults, collection, train_or_test, verbose=False):\n",
    "    for ind_name in GAresults:\n",
    "        entry = GAresults[ind_name]\n",
    "        entry['_id'] = ind_name #assign a unique _id to find it later\n",
    "        if train_or_test:\n",
    "            result = collection[train_or_test].insert_one(entry)\n",
    "        else:\n",
    "            result = collection.insert_one(entry)\n",
    "        if verbose==True:\n",
    "            print('One ind: {0}'.format(result.inserted_id))\n",
    " \n",
    "\n",
    "def convert_json_to_mongoDB_collection(jsonName):\n",
    "    ## Import JSON as a BSON\n",
    "    from bson import json_util\n",
    "    json_path = 'Genetic_Algorithm/GA_Results/'+jsonName+'.json'\n",
    "    with open(json_path, 'r') as f: \n",
    "            data = json_util.loads(f.read()) \n",
    "    # Insert each GA ind into MongoDB (have to do individually because can't insert files over 16MB at once)\n",
    "    collection = connect_to_mongoDB(jsonName)\n",
    "    for ind_name in data:\n",
    "        entry = data[ind_name]\n",
    "        entry['_id'] = ind_name #assign a unique _id to find it later\n",
    "        result = collection.insert_one(entry)\n",
    "        print('One ind: {0}'.format(result.inserted_id))\n",
    "convert_json_to_mongoDB_collection('GA_results_10gen_100inds')\n",
    "\n",
    "def extract_fields(field_list):\n",
    "    # get certain fields from every individual\n",
    "    field_query=[]\n",
    "    for field in field_list:\n",
    "        field_query += [\"'\",field,\"':1, \"]\n",
    "    FieldQuery = ''.join('{'+''.join(field_query)+'}')\n",
    "    fields_cursor = collection.find({}, eval(FieldQuery) )\n",
    "    return fields_cursor\n",
    "#fields_cursor = extract_fields(['generation','newBinary','fitness'])\n",
    "\n",
    "def mongoDB_to_df(field_list):\n",
    "    import pandas as pd\n",
    "    real = list(extract_fields(field_list))\n",
    "    return pd.DataFrame(real)\n",
    "# df = mongoDB_to_df(['fitness','newBinary','generation'])\n",
    " \n",
    "\n",
    "\"\"\"\n",
    "# Use MongoEngine to subset data in a way analagous to SQL\n",
    "import mongoengine as me\n",
    "me.connect('x2k_GA', host='localhost', port=27017)\n",
    "\n",
    "# Explore database    \n",
    "## Find specific individual  \n",
    "ind33gen4 = collection.find_one('ind33gen4')\n",
    "## Retrieve data subset    \n",
    "ind33gen4_nb = collection['train'].find_one({'_id':'ind33_gen4'})\n",
    "## Get a specific field from all entries\n",
    "field_list = ['newBinary','fitness']\n",
    " \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GA Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = 'GA_results_10gen_100inds'\n",
    "GAset = {'GMT_train':\"../X2K_Genetic_Algorithm/Validation/Perturbation_Data/GEO/Kinase_Perturbations_from_GEO_SUBSET1.80per.txt\",\n",
    "         'GMT_test':\"../X2K_Genetic_Algorithm/Validation/Perturbation_Data/GEO/Kinase_Perturbations_from_GEO_SUBSET2.20per.txt\",\n",
    "                    'gmtLimit':100, 'initial_pop_size':100, 'generations':10, 'select_fittest':10, 'selection_method':'fitnessProportional',\n",
    "                    'fitness_method':'target_shuffled_difference', 'children_per_generation':95, 'mutation_rate':.01, 'breeding_variation':0,\n",
    "                    'crossover_points':11, 'crossover_locations':'random','include_fittest_parents':5, 'threadPool_size':30,\n",
    "         'save_results':run_name}\n",
    "GAset = GAset_small = {'GMT_train':\"../X2K_Genetic_Algorithm/Validation/Perturbation_Data/GEO/Kinase_Perturbations_from_GEO_SUBSET1.80per.txt\",\n",
    "         'GMT_test':\"../X2K_Genetic_Algorithm/Validation/Perturbation_Data/GEO/Kinase_Perturbations_from_GEO_SUBSET2.20per.txt\",\n",
    "                    'gmtLimit':15, 'initial_pop_size':10, 'generations':3, 'select_fittest':4, 'selection_method':'fitnessProportional',\n",
    "                    'fitness_method':'target_shuffled_difference', 'children_per_generation':8, 'mutation_rate':.01, 'breeding_variation':0,\n",
    "                    'crossover_points':4, 'crossover_locations':'random','include_fittest_parents':2, 'threadPool_size':30,\n",
    "         'save_results':'GA_practice2'}\n",
    "\n",
    "def X2K_Web_GA_train(GAset):\n",
    "    # Prepare GMT input\n",
    "    from shutil import copyfile\n",
    "    clearTestGMT()\n",
    "    GMT = GAset['GMT_train']\n",
    "    copyfile(GMT, \"Genetic_Algorithm/testgmt/\"+ GMT.split(\"/\")[-1])\n",
    "    # Results Dicts\n",
    "    #all_GA_results={}\n",
    "    fitnessDict={}\n",
    "    # 0. Create initial population \n",
    "    population = createPopulation(GAset['initial_pop_size'])\n",
    "    # Loop over n generations\n",
    "    for gen in range(GAset['generations']):\n",
    "        print('================ GENERATION '+str(gen)+' ================')\n",
    "        # 1. Get all fitnesses \n",
    "        pop_fitness_results, fitnessDict = population_fitness(gen=gen, population=population, fitness_method=GAset['fitness_method'],\n",
    "                                                     fitnessDict=fitnessDict, gmtLimit=GAset['gmtLimit'], \n",
    "                                                              threadPool_size=GAset['threadPool_size'])\n",
    "        # Save gen results to MongoDB\n",
    "        collection = connect_to_mongoDB(GAset['save_results'], deleteOldDB=True)\n",
    "        GAresults_to_MongoDB(pop_fitness_results, collection, 'train', verbose=False)\n",
    "        #all_GA_results.update(pop_fitness_results)\n",
    "        ##all_GA_results = {**all_GA_results, **pop_fitness_results}\n",
    "        # 2. Select fittest\n",
    "        #fitnessDF = selectFittest(topNum=GAset['select_fittest'], pop_fitness_results=pop_fitness_results, selection_method=GAset['selection_method'])\n",
    "        fitnessDF = simple_selectFittest(topNum=GAset['select_fittest'], pop_fitness_results=pop_fitness_results)\n",
    "        # 3. Create/mutate children\n",
    "        newPopulation = createChildren(GAset['children_per_generation'], fitnessDF, GAset['mutation_rate'],\n",
    "                                       GAset['crossover_points'], GAset['crossover_locations'])\n",
    "        if GAset['include_fittest_parents'] > 0:\n",
    "            # When this is mixedTournament, selects from the parents that bred (regardless of whether they were the fittest in the whole population)\n",
    "            #newPopulation.extend( fitnessDF.sort_values(by='fitness')[:GAset['include_fittest_parents']]['newBinary'].toList()) \n",
    "            newPopulation = newPopulation + fitnessDF.sort_values(by='fitness')[:GAset['include_fittest_parents']]['newBinary'].tolist()\n",
    "        del population, pop_fitness_results\n",
    "        population = newPopulation\n",
    "        del newPopulation\n",
    " \n",
    "\n",
    "def X2K_Web_GA_test(GA_train, GAset):\n",
    "    test_GA_results={}\n",
    "    fitnessDict={}\n",
    "    GAtrain_DF = pd.DataFrame(GA_train).T\n",
    "    for gen in range(GAset['generations']):\n",
    "        population = GAtrain_DF.loc[GAtrain_DF['generation']==gen]['newBinary']\n",
    "        print('================ GENERATION '+str(gen)+' ================')\n",
    "        # 1. Get all fitnesses \n",
    "        pop_fitness_results, fitnessDict = population_fitness(gen=gen, population=population, fitness_method=GAset['fitness_method'],\n",
    "                                                     fitnessDict=fitnessDict, gmtLimit=GAset['gmtLimit'], threadPool_size=GAset['threadPool_size'])\n",
    "        #all_GA_results.update(pop_fitness_results)\n",
    "        test_GA_results = {**test_GA_results, **pop_fitness_results}\n",
    "    if GAset['save_results']!='No':\n",
    "        GAresults_to_MongoDB(test_GA_results, GAset, 'test')\n",
    "    return test_GA_results\n",
    "\n",
    "# GA_train = all_GA_results.copy()\n",
    "\n",
    "\n",
    "\n",
    "def time_estimator(GAset):\n",
    "    # Calculate average time it takes to run X2K on 50 GEO experiments\n",
    "    oneIndividual_50expts = [54.01,37.9,37.55,37.9,37.6,47.69,37.33,37.87,37.38,37.96]\n",
    "    seconds_per_experiment = (sum(oneIndividual_50expts)/len(oneIndividual_50expts))/50\n",
    "    if GAset['gmtLimit']!=False:\n",
    "        experiments = GAset['gmtLimit']\n",
    "    else:\n",
    "        experiments = 570\n",
    "    minutes  = (seconds_per_experiment * experiments * GAset['initial_pop_size'] * GAset['generations'])/60\n",
    "    minutes /= 16 # Speedup from using 20 cores = ~16x\n",
    "    hours = minutes/60 \n",
    "    print('GA will take: '+str(round(hours, 2))+' hours / '+str(round(minutes,2))+' minutes.')\n",
    "time_estimator(GAset)\n",
    "\n",
    "\n",
    "def GA_Train_Test(GAset):\n",
    "    import time\n",
    "    np.set_printoptions(threshold=5)\n",
    "\n",
    "    # Train GA\n",
    "    start = time.time()\n",
    "    GA_train = X2K_Web_GA_train(GAset) # GAset\n",
    "    end = time.time()\n",
    "    print(\"GA training took: \"+str(round(end - start, 2)/60/60/24)+\" days\")\n",
    "    \n",
    "\n",
    "    # Test GA  **** NEED TO HAVE TEST FUNCTION THAT USES POP FROM TRAINING\n",
    "    start = time.time()\n",
    "    GA_test = X2K_Web_GA_test(GA_train, GAset)\n",
    "    end = time.time()\n",
    "    print(\"GA testing took: \"+str(round(end - start, 2))+\"s\")\n",
    "    \n",
    "    # Save\n",
    "    GA_resultsDict = {'GA_train':GA_train, 'GA_test':GA_test, 'GA_settings':GAset}\n",
    "    collection = connect_to_mongoDB(GAset['save_results'], deleteOldDB=True)\n",
    "    GAresults_to_MongoDB(GA_resultsDict, collection, verbose=False)\n",
    "    \n",
    "    return GA_resultsDict\n",
    "\n",
    "GA_resultsDict = GA_Train_Test(GAset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run X2K Web Kinase Database Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#population of n individuals (one for each kinase db)\n",
    "kinase_DBs =[#kea 2016\n",
    "        'kea 2018',\n",
    "        'ARCHS4',\n",
    "        'iPTMnet',\n",
    "        'NetworkIN',\n",
    "        'Phospho.ELM',\n",
    "        'Phosphopoint',\n",
    "        'PhosphoPlus',\n",
    "        'MINT']\n",
    "\n",
    "best_options = {\n",
    "    'TF-target gene background database used for enrichment': [\n",
    "        # 'ChEA 2015',\n",
    "        # 'ENCODE 2015',\n",
    "        'ChEA & ENCODE Consensus',\n",
    "        # 'Transfac and Jaspar',\n",
    "        # 'ChEA 2016',\n",
    "        # 'ARCHS4 TFs Coexp',\n",
    "        # 'CREEDS',\n",
    "        # 'Enrichr Submissions TF-Gene Coocurrence',\n",
    "    ],\n",
    "    'kinase interactions to include': [#kea 2016\n",
    "        'kea 2018',\n",
    "        # 'ARCHS4',\n",
    "        # 'iPTMnet',\n",
    "        # 'NetworkIN',\n",
    "        # 'Phospho.ELM',\n",
    "        # 'Phosphopoint',\n",
    "        # 'PhosphoPlus',\n",
    "        # 'MINT',\n",
    "    ],\n",
    "    'enable_ppi': [\n",
    "        'ppid',\n",
    "        'Stelzl',\n",
    "        'IntAct',\n",
    "        'MINT',\n",
    "        'BioGRID',\n",
    "        # 'Biocarta',\n",
    "        # 'BioPlex',\n",
    "        # 'DIP',\n",
    "        # 'huMAP',\n",
    "        # 'InnateDB',\n",
    "        # 'KEGG',\n",
    "        # 'SNAVI',\n",
    "        # 'iREF',\n",
    "        # 'vidal',\n",
    "        # 'BIND',\n",
    "        # 'figeys',\n",
    "        # 'HPRD',\n",
    "    ],\n",
    "    'max_number_of_interactions_per_article': 1000000,\n",
    "    'max_number_of_interactions_per_protein': 200,\n",
    "    'min_network_size': 10,\n",
    "    'min_number_of_articles_supporting_interaction': 0,\n",
    "    'path_length': 2,\n",
    "    'included organisms in the background database': 'both',\n",
    "}\n",
    "\n",
    "    \n",
    "def create_kinaseDB_population(kinase_DBs):\n",
    "    rando_parameters = binary_to_parameters(createPopulation(1)[0])\n",
    "    for param in rando_parameters:\n",
    "        rando_parameters[param]['selection'] = best_options[param]\n",
    "    db_population=[]\n",
    "    for db in kinase_DBs:\n",
    "        new_bits=[]\n",
    "        for k in kinase_DBs:\n",
    "            if k==db:\n",
    "                new_bits.append('1')\n",
    "            else:\n",
    "                new_bits.append('0')\n",
    "        db_parameters = rando_parameters.copy()\n",
    "        db_parameters['kinase interactions to include']['selection'] = best_options['kinase interactions to include']\n",
    "        db_parameters['kinase interactions to include']['bits'] = ''.join(new_bits)\n",
    "        binary = parameters_to_binary(db_parameters)\n",
    "        db_population.append(binary)\n",
    "    return db_population\n",
    "db_population = create_kinaseDB_population(kinase_DBs)\n",
    "    \n",
    "\n",
    "kinaseDB_results, kinaseDB_fitnessDict = population_fitness(gen='KinaseDBs', population=db_population, fitnessDict={},\n",
    "                                     fitness_method='target_shuffled_difference', gmtLimit=50, threadPool_size=20)\n",
    " \n",
    "\n",
    "####### TEST INDIVIDUALS ON EVERY GMT EXPERIMENT \n",
    "# Best parameter combination from Random Search\n",
    "RStop_newBinary = '00100000100000001111100000000000000110110111'\n",
    "RandomSearch_results, RandomSearch_fitnessDict = population_fitness(gen='RandomSearch', population=[RStop_newBinary], fitnessDict={},\n",
    "                                     fitness_method='target_shuffled_difference', gmtLimit=False, threadPool_size=30)\n",
    "# Optimized individual from GA\n",
    "df = mongoDB_to_df(['fitness','newBinary','generation'])\n",
    "GAtop_newBinary = df.sort_values(by='fitness', ascending=False).copy()['newBinary'].iloc[0]\n",
    "GAtop_results, GAtop_fitnessDict = population_fitness(gen='Top_GA_individual', population=[GAtop_newBinary], fitnessDict={},\n",
    "                                     fitness_method='target_shuffled_difference', gmtLimit=False, threadPool_size=30)\n",
    "# Average individual from first gen of GA\n",
    "def get_average_individual(dat): \n",
    "    dat[['generation','fitness']] = dat[['generation','fitness']].apply(pd.to_numeric)\n",
    "    average_start_fitness = dat[dat['generation']==0]['fitness'].mean()\n",
    "    average_individual = dat.iloc[(dat['fitness']-average_start_fitness).abs().argsort()].iloc[0,:]\n",
    "    return average_individual\n",
    "GAavg_newBinary = get_average_individual(df)['newBinary']\n",
    "GAavg_results, GAavg_fitnessDict = population_fitness(gen='Top_GA_individual', population=[GAavg_newBinary], fitnessDict={},\n",
    "                                     fitness_method='target_shuffled_difference', gmtLimit=False, threadPool_size=30)\n",
    "# Save runs\n",
    "import pickle\n",
    "savePath = 'Genetic_Algorithm/GA_results/all_experiments_evaluations.p'\n",
    "pickle.dump( [RandomSearch_results, GAtop_results, GAavg_results], open( savePath, \"wb\" ) )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GA Results Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def plot_fitness(GA_train):\n",
    "    dat = pd.DataFrame(GA_train).T\n",
    "    dat[['generation','fitness']] = dat[['generation','fitness']].apply(pd.to_numeric)\n",
    "    dat['unique_id'] = dat.index\n",
    "    dat.groupby('generation').count()['fitness'] # Check that all gens have same # of individuals\n",
    "    # Get peak fitness and add back to parent df\n",
    "    dat_plot = dat.join(dat.groupby('generation')['fitness'].max(), on='generation', rsuffix='_peak').sort_values(by='unique_id')\n",
    "    #peak_dat = dat.groupby('generation')['unique_id', 'fitness'].max().reset_index()\n",
    "    #dat_plot['real_gen'] = dat_plot.index.str.split(\"_\").str[1].str.strip('gen').astype(int)\n",
    "    \n",
    "    # # Plot\n",
    "    # f, ax = plt.subplots(1, 1)\n",
    "    # sns.pointplot(data=dat_plot, x='generation', y='fitness', label='Mean Fitness', color='limegreen', ax=ax)\n",
    "    # sns.pointplot(data=dat_plot, x='generation', y='fitness_peak', label='Peak Fitness', color='forestgreen', markers='^', ax=ax)\n",
    "    # # Add legend\n",
    "    # ax.legend(handles=ax.lines[::len(dat_plot)+1], labels=[\"Fitness\",\"Fitness\"])\n",
    "    # leg_handles = ax.get_legend_handles_labels()[0]\n",
    "    # ax.legend(leg_handles, ['Blue', 'Orange'], title='New legend')\n",
    "\n",
    "    grp = dat_plot.groupby('generation')\n",
    "    fit_mean = grp['fitness'].mean().reset_index()\n",
    "    fit_error = grp['fitness'].std().reset_index()\n",
    "    fit_max = grp['fitness'].max().reset_index()\n",
    "    \n",
    "    f, ax = plt.subplots(1, 1)\n",
    "    ax.errorbar(data=fit_mean, x='generation', y='fitness', yerr=fit_error['fitness'], label='Mean Fitness',\n",
    "                 marker='o', markersize=4, color='limegreen', capsize=2)\n",
    "    ax.errorbar(data=fit_max, x='generation', y='fitness', label='Peak Fitness', color='forestgreen', marker='^',  markersize=4)\n",
    "    ax.legend(loc='lower right')\n",
    "    plt.xticks(np.arange(min(fit_mean['generation']), max(fit_mean['generation'])+1, 2))\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Fitness')\n",
    "    #ax.legend(handles=ax.lines[2:] , labels=[\"Mean Fitness\",\"Peak Fitness\" ])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assess Optimal Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topParameters_to_excel(top_individual):\n",
    "    optimal_params = binary_to_parameters(top_individual['newBinary'])\n",
    "    finalDict={}\n",
    "    for key in optimal_params:\n",
    "        selection=optimal_params[key]['selection']\n",
    "        print(key+' : '+str(selection))\n",
    "        finalDict[key] = selection\n",
    "    pd.Series(finalDict).to_excel('optimized_parameters.xlsx')\n",
    "    return finalDict\n",
    "\n",
    "def percent_kinases_recovered(individual):\n",
    "    kea = pd.DataFrame(individual['KEA_results'])\n",
    "    keaDict={}\n",
    "    for col in kea:\n",
    "        kcol = kea[col].copy()\n",
    "        target = col.split(\"_\")[0]\n",
    "        match = kcol[kcol.index==target]\n",
    "        if len(match)>0:\n",
    "            rank=match.values[0] \n",
    "        else:\n",
    "            rank=np.nan \n",
    "        keaDict[col] = rank\n",
    "    Ranks = keaDict.values() \n",
    "    # Regardless of p-val\n",
    "    NAbool = [x for x in Ranks if x is not np.nan ]\n",
    "    percent = len(NAbool)/len(Ranks)\n",
    "    # Taking into account pvalue\n",
    "    Ranks_filt = [x for x in Ranks if x is not np.nan and x<.05]\n",
    "    percent_sig = len(Ranks_filt)/len(Ranks)\n",
    "    return Ranks, percent*100, percent_sig*100\n",
    "\n",
    "\n",
    "Ranks_RStop, percent_RStop, percentSig_RStop = percent_kinases_recovered(next(iter(RandomSearch_results.values())) )\n",
    "Ranks_GAtop, percent_GAtop, percentSig_GAtop = percent_kinases_recovered(next(iter(GAtop_results.values())) )\n",
    "Ranks_GAavg, percent_GAavg, percentSig_GAavg = percent_kinases_recovered(next(iter(GAavg_results.values())) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot top individual's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def KDE_multiplot(individual, scaledRanks=True, supTitle=''):\n",
    "    # Get ranks\n",
    "    try:\n",
    "        shuffled_ranks = individual['shuffled_ranks']\n",
    "    except:\n",
    "        shuffled_ranks = individual['shuffled_mean_ranks']\n",
    "    target_ranks = individual['target_ranks']\n",
    "    \n",
    "    zScores = values_to_zscores( pd.DataFrame(individual['KEA_results']) )\n",
    "    nExperiments = len(zScores.columns)\n",
    "    Ranks, percent, percent_sig  = percent_kinases_recovered(individual)\n",
    "    rank_matrix = values_to_ranks(pd.DataFrame(individual['KEA_results']))\n",
    "    Zranks_matrix = values_to_ranks(zScores)\n",
    "    # Plot\n",
    "    #ax = plt.subplots(111)\n",
    "    plt.figure()\n",
    "    Zstack = Zranks_matrix.stack().reset_index()\n",
    "    Zstack.columns = ['Kinase','Experiment','Rank']\n",
    "    if scaledRanks==True:\n",
    "        DFstack = scaled_ranks(Zstack) \n",
    "    ## Null distribution (all kinase ranks)\n",
    "    g0 = sns.distplot( Zstack['Rank'], label='All Kinases',rug=False, hist=False, norm_hist=True ).set_xlim(0,1) \n",
    "    \n",
    "    ## Target Kinases Only: Zscore ranks\n",
    "    g1 = sns.distplot( target_ranks, label='Target Kinases',rug=False, hist=True, norm_hist=True ).set_xlim(0,1)\n",
    "    ## Target Kinases Only: raw ranks\n",
    "    g2 = sns.distplot( target_ranks, label='Target Kinases',rug=False, hist=True, norm_hist=True ).set_xlim(0,1)\n",
    "    \n",
    "    ## Shuffled targets\n",
    "    g3 = sns.distplot( shuffled_ranks, label='Shuffled Kinases',rug=False, hist=True, norm_hist=True,\n",
    "                  kde_kws={\"linestyle\":\"--\"}).set_xlim(0,1)\n",
    "   \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlim(0,1)\n",
    "    plt.ylim(0,4)\n",
    "    plt.title(supTitle)\n",
    "    plt.text(.01,3.9,0,text='Fitness = '+str(round(individual['fitness'],2))+'\\nKinases recovered = '+str(round(percent,2))+' %'+\\\n",
    "                           '\\nSig. kinases recovered = '+str(round(percent_sig,2))+' %'+'\\n'+\"Experiments = \" +str(nExperiments),\n",
    "             horizontalalignment='left', verticalalignment='top') \n",
    "    return g2\n",
    "\n",
    "# Top fittest individual from GA\n",
    "## Top parameter combination from Random Search\n",
    "KDE_multiplot(individual=pd.Series(next(iter(RandomSearch_results.values()))), scaledRanks=True, supTitle='Random Search: Optimized')\n",
    "##  Top ind from GA\n",
    "KDE_multiplot(individual=pd.Series(next(iter(GAtop_results.values()))), scaledRanks=True, supTitle='X2K GA: Optimized')\n",
    "## Average ind from GA\n",
    "KDE_multiplot(individual=pd.Series(next(iter(GAavg_results.values()))), scaledRanks=True, supTitle='X2K GA: Average Individual')\n",
    "\n",
    "\n",
    "f, AX = plt.subplots(2,int(len(kinaseDB_results)/2))\n",
    "AX = AX.ravel()\n",
    "\n",
    "count=0\n",
    "for key in kinaseDB_results:\n",
    "    g = KDE_multiplot(individual=kinaseDB_results[key], scaledRanks=True, saveFig=False, supTitle=kinase_DBs[count])\n",
    "    count+=1\n",
    "\n",
    "kinaseDB_results['ind1_genKinaseDBs']['target_ranks']\n",
    "kinaseDB_results['ind1_genKinaseDBs']['shuffled_ranks']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot G2N Subnetwork size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subnetwork_size_evolution(GA_train):\n",
    "    DF = pd.DataFrame(GA_train).T\n",
    "    DF['G2N_results'].apply(lambda x: )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Search comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
